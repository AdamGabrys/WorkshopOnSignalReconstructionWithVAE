{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build VAE model\n",
    "**In this notebook we will build a Bayesian Autoencoder for sinusoid signal reconstruction.**\n",
    "\n",
    "<img src=\"misc/SpeechReconstruction.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img src=\"misc/vae_vis2.png\" alt=\"Drawing\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import rnn, nn\n",
    "from mxnet import ndarray as nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder implementation. \n",
    "We will implement encoder as a simple 1 layer [GRU](https://mxnet.incubator.apache.org/api/python/gluon/rnn.html#mxnet.gluon.rnn.GRU) with the number of features in the hidden state h equal to 10. We will treat last GRU state as a sinusoid signall encoding. So Encoder lattent space will  10 dimensions.\n",
    "<img src=\"misc/GRU_image.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img src=\"misc/encoder_vis.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinEncoder(gluon.Block):\n",
    "    \"\"\"\n",
    "    Sinus GRU encoder. Last GRU state is used as an encoded representation of the sinus signal.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SinEncoder, self).__init__(**kwargs)\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.rnn = rnn.GRU(hidden_size=10)\n",
    "\n",
    "    def forward(self, signal: nd.ndarray):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            signal: Sin signal (m, signal_lengths), m - num of signals (batch_size)\n",
    "\n",
    "        Returns: (last_gru_layer_output:Array(m,s) where m - batch size, s - dimensionality of the GRU latent space\n",
    "\n",
    "        \"\"\"\n",
    "        output = nd.transpose(signal, axes=(1, 0))  # (length, m)\n",
    "        output = output.expand_dims(axis=2)  # (length, m, 1)\n",
    "\n",
    "        initial_hidden_state = self.rnn.begin_state(func=nd.zeros, batch_size=signal.shape[0])\n",
    "        output, latent_space = self.rnn(output, initial_hidden_state)\n",
    "\n",
    "        output = output[-1, :]  # (m,hidden_size)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize our Encoder structure. It should encode any sequence to 10 dimensional space. Lets check if it is true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SinEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be confused with `TNC`. It is the format of input and output tensors. T, N and C stand for sequence length, batch size, and feature dimensions respectively.\n",
    "\n",
    "### Lets feedforward some data through the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SinEncoder()\n",
    "encoder.initialize()\n",
    "encoder(nd.ones((4, 50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder implementation\n",
    "Decoder provide functionality to reconstruct signal with respect to parametrization sample drawn from isotropic Gaussian latent space.\n",
    "\n",
    "We will implement decoder as a single layer [GRU](https://mxnet.incubator.apache.org/api/python/gluon/rnn.html#mxnet.gluon.rnn.GRU), followed with projection layer implemented as Fully Connected DNN with linear activation function. Fixed dimensional space will be decoded into sequence of signal frames.\n",
    "\n",
    "We will do that in the loop. During inference in each step we will predict 5 frames of the reconstructed signal. This signal frames will be concatenated with sample drawn from Gaussian latent space and feed to the GRU as an input in the following step.\n",
    "\n",
    "During the training we will use teacher-forcing, it will help our decoder converge quicker. It means that sometimes instead of feeding predicted frames of the reconstructed signal, we will select reference signal frames from oracle signal. Whether to take predicted or original frames will be decided with constant probability.\n",
    "\n",
    "Decoder will implement following flow:\n",
    "<img src=\"misc/decoder_vis2.png\" alt=\"Drawing\" style=\"height: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatefulGRUCell:\n",
    "    \"\"\"\n",
    "    Wrapper over GRUCell caching hidden states.\n",
    "    \"\"\"\n",
    "    def __init__(self, gru_cell: rnn.GRUCell):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gru_cell: GRUCell which will be used to run forward pass.\n",
    "            **kwargs:\n",
    "        \"\"\"\n",
    "        self._gru_cell = gru_cell\n",
    "        self._state = None\n",
    "\n",
    "    def __call__(self, input_: nd.NDArray):\n",
    "        \"\"\"\n",
    "        Run forward pass on GRUCell.\n",
    "        First forward call will be initialized with GRUCell.begin_state, all further calls\n",
    "        will use the latest state returned by GRUCell.\n",
    "        Args:\n",
    "            input_: input tensor with shape (batch_size, input_size).\n",
    "        \"\"\"\n",
    "        if not self._state:\n",
    "            self._state = self._gru_cell.begin_state(input_.shape[0])\n",
    "        output, new_state = self._gru_cell(input_, self._state)\n",
    "        self._state = new_state\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinDecoder(gluon.Block):\n",
    "\n",
    "    def __init__(self, samples_per_step: int = 5, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            samples_per_step: How many samples to generate at each decoding step.\n",
    "            **kwargs:\n",
    "        \"\"\"\n",
    "        super(SinDecoder, self).__init__(**kwargs)\n",
    "\n",
    "        self._samples_per_step = samples_per_step\n",
    "\n",
    "        with self.name_scope():\n",
    "            self._decoder_rnn = rnn.GRUCell(hidden_size=20)\n",
    "            self._projection_layer = nn.Dense(samples_per_step, flatten=False)\n",
    "\n",
    "    def forward(self, gaussian_latent_space_sample: nd.NDArray,\n",
    "                length: int, target_signal: nd.NDArray,\n",
    "                teacher_forcing_prob: float):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gaussian_latent_space_sample: (batch_size, 1)\n",
    "            length: The total number of samples to generate\n",
    "            target_signal: True sinus signal that is used during teacher-forcing training.\n",
    "            teacher_forcing_prob: The probability of using the teacher forcing.\n",
    "\n",
    "        Returns:\n",
    "            Decoder sinus signal: (batch_size, length)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = gaussian_latent_space_sample.shape[0]\n",
    "        stateful_rnn = StatefulGRUCell(self._decoder_rnn)\n",
    "        predicted_frame = mx.nd.zeros(shape=(batch_size, self._samples_per_step))\n",
    "        reconstructed_signal = [] \n",
    "        for i in range(length // self._samples_per_step):\n",
    "            if target_signal is not None:\n",
    "                predicted_frame = self._select_signal(predicted_frame, target_signal,\n",
    "                                                      teacher_forcing_prob, i)  # teacher forcing\n",
    "            rnn_input = nd.concat(gaussian_latent_space_sample,\n",
    "                                  predicted_frame, dim=1)  # (batch_size, samples_per_step + 1)\n",
    "            rnn_output = stateful_rnn(rnn_input)  # (batch_size, rnn_hidden_size)\n",
    "            predicted_frame = self._projection_layer(rnn_output)  # (batch_size, samples_per_step)\n",
    "            reconstructed_signal.append(predicted_frame)\n",
    "\n",
    "        return nd.concat(*reconstructed_signal, dim=1)  # (batch_size, floor(length/samples_per_step) * samples_per_step)\n",
    "\n",
    "    def _select_signal(self,\n",
    "                       predicted_signal: nd.NDArray,\n",
    "                       oracle_signal: nd.NDArray,\n",
    "                       teacher_forcing_prob: float, \n",
    "                       step: int):\n",
    "        \"\"\"\n",
    "        Base on teacher_forcing_prob select either predicted_signal or aligned signal from\n",
    "        Oracle signal.\n",
    "        Args:\n",
    "            predicted_signal: (batch_size, _samples_per_step)\n",
    "            oracle_signal: (batch_size, #total number of samples)\n",
    "            teacher_forcing_prob: probability that oracle signal will be selected\n",
    "            step: step used to align between oracle and predicted signal\n",
    "        Returns:\n",
    "            signal for forward pass\n",
    "        \"\"\"\n",
    "        if step > 0 and np.random.rand() < teacher_forcing_prob:\n",
    "            start_sample = (step - 1) * self._samples_per_step\n",
    "            stop_sample = start_sample + self._samples_per_step\n",
    "            teacher_forcing_signal = oracle_signal[:, start_sample:stop_sample]\n",
    "            return teacher_forcing_signal\n",
    "        else:\n",
    "            return predicted_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to visualize the decoder we have created. It should consist of two steps, one is decoder rnn which takes lattent variable and previously predicted sequence frames. And the other one is projection_layer which will predict next 5 frames from decoder_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SinDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = SinDecoder()\n",
    "decoder.initialize()\n",
    "decoder(nd.ones((2, 1)), 50, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Put all the pices toogether into Gaussian Variational Autoencoder\n",
    "\n",
    "Now when both encoder and decoder are already defined, lets put all the pices toogether adding final layer responsible for Gaussian lattent space embedding\n",
    "\n",
    "To compute Gaussian lattent space we will use Fully Connected layer with 2 outputs, representing variance and mean of isotropic Gaussian. These values are predicted from Encoder embeddings.\n",
    "\n",
    "Additionally we will implement functionality to override this predicted value during inference.\n",
    "\n",
    "Apart of this we will also add to our model `calc_loss` function which is responsible for computation of L2 and negative KLD loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinBayesianAutoEncoder(gluon.Block):\n",
    "\n",
    "    def __init__(self, n_latent=2, **kwargs):\n",
    "        super(SinBayesianAutoEncoder, self).__init__(**kwargs)\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.encoder = SinEncoder()\n",
    "            self.latent_space = nn.Dense(n_latent * 2)  # mean and log variance of the latent space\n",
    "            self.decoder = SinDecoder(samples_per_step=5)  # predict 5 samples at once\n",
    "            self.l2loss = gluon.loss.L2Loss()\n",
    "\n",
    "    def forward(self, signal: nd.NDArray, teacher_forcing_prob: float,\n",
    "                latent_space_override: nd.NDArray = None):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            signal: Sin signal (m, signal_length), m - num of signals (batch_size)\n",
    "            teacher_forcing_prob: The probability of activating the teacher forcing\n",
    "            latent_space_override: The override value for the latent space.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        sig_embedding = self.encoder(signal)  # (m,s), s - dim of the encoder embedding\n",
    "\n",
    "        # Posterior of the latent space\n",
    "        # Gaussian variance must be positive, therefore using log variance parametrization\n",
    "        ls_mean, ls_log_var = self.latent_space(sig_embedding).split(axis=1, num_outputs=2)\n",
    "        ls_std = nd.exp(ls_log_var * 0.5, axis=0)\n",
    "\n",
    "        # Sampling from the unit gaussian instead of sampling from the latent space posterior\n",
    "        # allow for gradient flow via latent_space_mean / latent_space_log_var parameters\n",
    "        # z = (x-mu)/std, thus: x = mu + z*std\n",
    "        normal_sample = nd.random_normal(0, 1, shape=ls_mean.shape)\n",
    "        ls_val = ls_mean + ls_std * normal_sample\n",
    "\n",
    "        if isinstance(latent_space_override, nd.NDArray):\n",
    "            ls_val = latent_space_override\n",
    "\n",
    "        length = signal.shape[1]\n",
    "        reconstructed_sig = self.decoder(ls_val, length, signal, teacher_forcing_prob)  # (m,length)\n",
    "\n",
    "        return SinBAEOutput(ls_mean, ls_log_var, ls_val, reconstructed_sig)\n",
    "\n",
    "    def calc_loss(self, signal: nd.NDArray, teacher_forcing_prob: float) -> (float, float):\n",
    "        \"\"\"\n",
    "        Compute gradients of the loss function with respect of the model parameters.\n",
    "\n",
    "        Args:\n",
    "            signal: Sin signal: (m, signal_length), m - num of signals (batch_size)\n",
    "            \n",
    "        Returns: L2 Loss between input and decoded signals, KLD loss\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        decoded_signal_output = self(signal, teacher_forcing_prob)\n",
    "\n",
    "        latent_space_mean = decoded_signal_output.latent_space_mean\n",
    "        latent_space_log_var = decoded_signal_output.latent_space_log_var\n",
    "\n",
    "        l2_loss = self.l2loss(signal, decoded_signal_output.decoded_signal)\n",
    "        negative_kld = 0.5 * nd.sum(\n",
    "            1 + latent_space_log_var - latent_space_mean ** 2 - nd.exp(latent_space_log_var), axis=1)\n",
    "\n",
    "        return l2_loss, -negative_kld\n",
    "\n",
    "\n",
    "class SinBAEOutput(NamedTuple):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        latent_space_mean: array(m,1), The mean of the posterior of the latent space\n",
    "        latent_space_log_var: array(m,1), Variance of the posterior of the latent space\n",
    "        latent_space_val: array(m,1), Value sampled from the posterior of the latent space\n",
    "        decoded_signal: array(m,signal_length)\n",
    "    \"\"\"\n",
    "    latent_space_mean: nd.NDArray\n",
    "    latent_space_log_var: nd.NDArray\n",
    "    latent_space_val: nd.NDArray\n",
    "    decoded_signal: nd.NDArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create actual model\n",
    "Now when all building blocks are in place, let's create SinBayesianAutoEncoder instance and visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "SinBayesianAutoEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = SinBayesianAutoEncoder()\n",
    "vae.initialize()\n",
    "vae(nd.ones((4, 50)), 0.6, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
