{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traine Varational Auto Encoder Model\n",
    "In this notebook we will train variational auto encoder model, which we have created in the previous notebook. We will train model on data generated with signal generator from our first notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "\n",
    "from typing import List\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import mxnet.ndarray as nd\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mlss_gdansk2019.SignalGenerator import SignalGenerator\n",
    "from mlss_gdansk2019.SinBayesianAutoEncoderModel import SinBayesianAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to have repetable results\n",
    "np.random.seed(54545)\n",
    "\n",
    "# Logging settings\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Trainer class\n",
    "Trainer will be responsible for running forward and bacward pass over the model we have created. It will use adam optimizer to update weights of our model. Trainer will compute both train and test loss of both KLD and L2 loss.\n",
    "\n",
    "**DataLoaders** are used to create mini-batches of samples from a Dataset, and provides a convenient iterator interface for looping these batches. Itâ€™s typically much more efficient to pass a mini-batch of data through a neural network than a single sample at a time, because the computation can be performed in parallel. A required parameter of DataLoader is the size of the mini-batches you want to create, called batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinBayesianAutoEncoderTrainer:\n",
    "    \"\"\"\n",
    "    Train SinBayesianAutoEncoder model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 train_dataloader: DataLoader,\n",
    "                 test_dataloader: DataLoader,\n",
    "                 model: SinBayesianAutoEncoder,\n",
    "                 kld_weight: float = 0.01,\n",
    "                 optimizer: str = \"adam\",\n",
    "                 learning_rate: float = 0.001):\n",
    "        self._train_dataloader = train_dataloader\n",
    "        self._test_dataloader = test_dataloader\n",
    "        self._model = model\n",
    "        self._kld_weight = kld_weight\n",
    "        self._trainer = gluon.Trainer(self._model.collect_params(),\n",
    "                                      optimizer, {'learning_rate': learning_rate})\n",
    "        self.logger = logging.getLogger()\n",
    "        self.logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    def train(self, n_epoch: int, decay_multiplier: int = 0.01) -> (List[float],\n",
    "                                                                    List[float],\n",
    "                                                                    List[float]):\n",
    "        \"\"\"\n",
    "        Train the model for given number of epochs updating its weights,\n",
    "        and exponentially decaying teacher forcing probability\n",
    "        Args:\n",
    "            n_epoch: Number of epochs for which model will be trained.\n",
    "            decay_multiplier: multiplier of epoch in exponential decay of\n",
    "                              teacher forcing.\n",
    "\n",
    "        Returns: test_l2_loss_list, test_kld_list, teacher_forcing_prob_list\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "\n",
    "        test_l2_loss_list = []\n",
    "        test_kld_list = []\n",
    "        teacher_forcing_prob_list = []\n",
    "        for epoch in range(n_epoch):\n",
    "            teacher_forcing_prob = np.exp(-decay_multiplier * epoch)\n",
    "\n",
    "            epoch_l2_train_loss, epoch_kld_train_loss = self._forward_backward(teacher_forcing_prob)\n",
    "            epoch_l2_test_loss, epoch_kld_test_loss = self._calc_test_accuracy(teacher_forcing_prob)\n",
    "\n",
    "            epoch_total_train_loss = epoch_l2_train_loss + epoch_kld_train_loss\n",
    "            epoch_total_test_loss = epoch_l2_test_loss + epoch_kld_test_loss\n",
    "            self.logger.info(\n",
    "                'Epoch %d, teacher_forcing_prob=%.2f, training loss (total/l2/kld): '\n",
    "                '%.4f, %.4f, %.4f, test loss (total/l2/kld): %.4f, %.4f, %.4f'\n",
    "                % (epoch,\n",
    "                   teacher_forcing_prob,\n",
    "                   epoch_total_train_loss, epoch_l2_train_loss, epoch_kld_train_loss,\n",
    "                   epoch_total_test_loss, epoch_l2_test_loss, epoch_kld_test_loss))\n",
    "\n",
    "            test_l2_loss_list.append(epoch_l2_test_loss)\n",
    "            test_kld_list.append(epoch_kld_test_loss)\n",
    "            teacher_forcing_prob_list.append(teacher_forcing_prob)\n",
    "\n",
    "        end = time.time()\n",
    "        self.logger.info('Time elapsed: {:.2f}s'.format(end - start))\n",
    "\n",
    "        return test_l2_loss_list, test_kld_list, teacher_forcing_prob_list\n",
    "\n",
    "    def _forward_backward(self, teacher_forcing_prob: float) -> (float, float):\n",
    "        \"\"\"\n",
    "        Run single epoch forward and backward path with update of parameters on the model.\n",
    "        Args:\n",
    "            teacher_forcing_prob: probability of selecting oracle signal\n",
    "        Returns:\n",
    "            l2 loss, kld loss\n",
    "        \"\"\"\n",
    "        epoch_l2_loss = 0\n",
    "        epoch_kld_loss = 0\n",
    "\n",
    "        for signal_batch in self._train_dataloader:\n",
    "            with autograd.record():\n",
    "                l2_loss, kld_loss = self._model.calc_loss(signal_batch, teacher_forcing_prob)\n",
    "                loss = l2_loss + self._kld_weight * kld_loss  # weighting the kld loss\n",
    "\n",
    "            loss.backward()\n",
    "            self._trainer.step(signal_batch.shape[0])\n",
    "\n",
    "            epoch_l2_loss += nd.mean(l2_loss).asscalar()\n",
    "            epoch_kld_loss += nd.mean(kld_loss).asscalar()\n",
    "\n",
    "        epoch_l2_loss /= len(self._train_dataloader)\n",
    "        epoch_kld_loss /= len(self._train_dataloader)\n",
    "\n",
    "        return epoch_l2_loss, epoch_kld_loss\n",
    "\n",
    "    def _calc_test_accuracy(self, teacher_forcing_prob: float) -> (float, float):\n",
    "        \"\"\"\n",
    "        Computes test set single epoch loss.\n",
    "        Args:\n",
    "            teacher_forcing_prob: probability of selecting oracle signal\n",
    "        Returns:\n",
    "            l2 loss, kld loss\n",
    "        \"\"\"\n",
    "        epoch_l2_loss = 0\n",
    "        epoch_kld_loss = 0\n",
    "\n",
    "        for signal_batch in self._test_dataloader:\n",
    "            l2_loss, kld_loss = self._model.calc_loss(signal_batch, teacher_forcing_prob)\n",
    "            kld_loss = self._kld_weight * kld_loss\n",
    "\n",
    "            epoch_l2_loss += nd.mean(l2_loss).asscalar()\n",
    "            epoch_kld_loss += nd.mean(kld_loss).asscalar()\n",
    "\n",
    "        epoch_l2_loss /= len(self._test_dataloader)\n",
    "        epoch_kld_loss /= len(self._test_dataloader)\n",
    "\n",
    "        return epoch_l2_loss, epoch_kld_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Almost there. Now we can train the model we have  created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mx.Context(mx.cpu()):\n",
    "    # Generate n sin functions (both train and test sets)\n",
    "    signal_generator = SignalGenerator()\n",
    "    multi, phase, train_signals = signal_generator.generate_signals(50, 500)\n",
    "    _, _, test_signals = signal_generator.generate_signals(50, 50)\n",
    "    signal_generator.plot_signals_with_multipliers(multi, phase, train_signals)\n",
    "    \n",
    "    # Create sin auto encoder model\n",
    "    sin_bae = SinBayesianAutoEncoder()\n",
    "    sin_bae.initialize(mx.init.Xavier())\n",
    "\n",
    "    # Train the model and save to disk\n",
    "    train_data_loader = DataLoader(nd.array(train_signals), batch_size=20, shuffle=True)\n",
    "    test_data_loader = DataLoader(nd.array(test_signals), batch_size=20, shuffle=False)\n",
    "    sin_bae_trainer = SinBayesianAutoEncoderTrainer(train_data_loader,\n",
    "                                                   test_data_loader,\n",
    "                                                   sin_bae)\n",
    "    test_l2_loss_list, test_kld_list, teacher_forcing_prob_list = sin_bae_trainer.train(n_epoch=1000)\n",
    "    sin_bae.save_parameters(\"model/model_train.params\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the training loss\n",
    "Notice how kld and l2 loss behaves. Do you know what might have caused that spike?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training loss\n",
    "total_loss = list(np.array(test_kld_list) + np.array(test_l2_loss_list))\n",
    "ax = plt.subplot()\n",
    "ax.plot(np.arange(len(test_l2_loss_list)), test_l2_loss_list, label=\"l2 loss\")\n",
    "ax.plot(np.arange(len(test_kld_list)), test_kld_list, label=\"kld loss\")\n",
    "ax.plot(np.arange(len(total_loss)), total_loss, label=\"total loss\")\n",
    "ax.set_xlabel(\"iter\", fontsize=14)\n",
    "ax.set_ylabel(\"loss\", fontsize=14)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
